{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn            # containing various building blocks for your neural networks\n",
    "import torch.optim as optim      # implementing various optimization algorithms\n",
    "import torch.nn.functional as F  # a lower level (compared to torch.nn) interface\n",
    "\n",
    "# torchvision: popular datasets, model architectures, and common image transformations for computer vision.\n",
    "import torchvision\n",
    "# transforms: transformations useful for image processing\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import glob\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from collections import  Counter\n",
    "#from torchtext import data\n",
    "import gensim.downloader as api\n",
    "from sklearn.metrics import fbeta_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load caption embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30000, 24, 25])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_embeddings = np.load('nlp_embeddings.npy')\n",
    "seq_embeddings = torch.from_numpy(seq_embeddings)\n",
    "seq_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9253, -0.3388, -0.3214,  ..., -0.5276, -1.2592, -0.4511],\n",
       "         [-1.0690,  0.1474,  0.1558,  ...,  0.9571, -0.3524,  0.8515],\n",
       "         [-0.6932, -0.8863, -0.6497,  ...,  0.6270, -0.2331, -0.6294],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.1313,  0.1184,  0.1065,  ..., -0.5846, -0.7263, -1.2605],\n",
       "         [ 0.0538,  0.2227, -1.0852,  ..., -0.5067, -0.0436,  0.5601],\n",
       "         [-1.8058,  0.6504,  0.6644,  ...,  1.1326, -1.5864,  0.1625],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.6982, -0.0865, -0.3894,  ..., -0.7515, -0.9934,  0.1908],\n",
       "         [-1.8058,  0.6504,  0.6644,  ...,  1.1326, -1.5864,  0.1625],\n",
       "         [-0.8464, -0.1723,  0.2490,  ...,  0.2351, -0.7627,  1.0823],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.3701, -0.3965, -0.0217,  ..., -0.2388,  0.2475,  0.7639],\n",
       "         [-1.8058,  0.6504,  0.6644,  ...,  1.1326, -1.5864,  0.1625],\n",
       "         [-0.9167, -0.3292, -0.0541,  ...,  0.8347, -0.7725,  0.5756],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.4331, -0.9173,  0.0289,  ..., -0.2076, -0.9349, -0.7067],\n",
       "         [ 1.1687, -0.0886,  0.8875,  ..., -0.3564, -0.6216, -0.0654],\n",
       "         [ 0.6153,  0.4090, -0.3422,  ..., -0.4323, -0.9998, -0.6808],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.5830,  0.4452,  1.0406,  ...,  0.1474, -0.6840, -0.5839],\n",
       "         [-0.4528,  0.6963, -0.0939,  ..., -0.7787, -0.9789,  0.0883],\n",
       "         [ 0.0538,  0.2227, -1.0852,  ..., -0.5067, -0.0436,  0.5601],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_label = pd.read_csv('train.csv')\n",
    "train_label = np.array(df_train_label[['Labels']])\n",
    "train_label_list = []\n",
    "for i in train_label:\n",
    "    for k in i:\n",
    "        train_label_list.append([int(j) for j in k.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 1],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(train_label_list)\n",
    "y_train = mlb.transform(train_label_list)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 13, 14, 15, 16, 17, 18,\n",
       "       19])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_t = torch.FloatTensor(y_train)\n",
    "y_train_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2data(Dataset):\n",
    "    \"\"\"\n",
    "    A customized data loader for MNIST.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 root,labels,seq_embeddings,\n",
    "                 transform=None,\n",
    "                 preload=False, test=False):\n",
    "        \"\"\" Intialize the MNIST dataset\n",
    "        \n",
    "        Args:\n",
    "            - root: root directory of the dataset\n",
    "            - tranform: a custom tranform function\n",
    "            - preload: if preload the dataset into memory\n",
    "        \"\"\"\n",
    "        self.images = None\n",
    "        self.labels = labels\n",
    "        self.seq_embeddings = seq_embeddings\n",
    "        self.filenames = []\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "            \n",
    "        # read filenames\n",
    "        if test == False:\n",
    "            for i in range(30000):\n",
    "                filename = self.root + str(i) + '.jpg'\n",
    "                self.filenames.append(filename)\n",
    "        else:\n",
    "            for i in range(30000,40000):\n",
    "                filename = self.root + str(i) + '.jpg'\n",
    "                self.filenames.append(filename)            \n",
    "\n",
    "        # if preload dataset into memory\n",
    "        if preload:\n",
    "            self._preload()\n",
    "            \n",
    "        self.len = len(self.filenames)\n",
    "                              \n",
    "    def _preload(self):\n",
    "        \"\"\"\n",
    "        Preload dataset to memory\n",
    "        \"\"\"\n",
    "        self.images = []\n",
    "        for image_fn in self.filenames:            \n",
    "            # load images\n",
    "            image = Image.open(image_fn)\n",
    "            self.images.append(image.copy())\n",
    "            # avoid too many opened files bug\n",
    "            image.close()\n",
    "\n",
    "    # probably the most important to customize.\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset\n",
    "        \"\"\"\n",
    "        # If dataset is preloaded\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        embeddings = self.seq_embeddings[index]\n",
    "        # May use transform function to transform samples\n",
    "        # e.g., random crop, whitening\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        # return image and label\n",
    "        return image, label, embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.CenterCrop(32),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = [0.485, 0.456, 0.406],std = [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = A2data(\n",
    "    root='data/',labels=y_train_t,seq_embeddings=seq_embeddings,\n",
    "    preload=True, transform=data_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.3027,  0.0741,  0.2111,  ..., -0.3198, -0.3198, -0.3369],\n",
       "          [-0.5082,  0.0056,  0.1939,  ...,  1.0673,  1.0502,  1.0159],\n",
       "          [-0.6281,  0.0912,  0.5022,  ...,  0.8447,  0.7933,  0.7248],\n",
       "          ...,\n",
       "          [ 0.5193,  0.5878,  0.6049,  ...,  0.6906,  0.1597,  0.0056],\n",
       "          [ 0.4166,  0.4337,  0.3994,  ...,  1.3584, -0.2856, -1.1932],\n",
       "          [-0.1486, -0.4226, -0.6623,  ...,  1.1015,  0.5193, -1.1075]],\n",
       " \n",
       "         [[ 1.0630,  1.2556,  1.2906,  ...,  0.8354,  0.8179,  0.8179],\n",
       "          [ 0.9755,  1.2206,  1.2906,  ...,  1.4482,  1.4307,  1.4132],\n",
       "          [ 0.8880,  1.2556,  1.4132,  ...,  1.2031,  1.1856,  1.1331],\n",
       "          ...,\n",
       "          [ 1.5007,  1.5357,  1.5182,  ...,  1.3256,  1.0455,  0.8880],\n",
       "          [ 1.4832,  1.4832,  1.4307,  ...,  1.6232,  0.3277, -0.4951],\n",
       "          [ 0.8354,  0.4853,  0.2227,  ...,  1.3782,  1.0455, -0.2850]],\n",
       " \n",
       "         [[ 1.3851,  1.4897,  1.5245,  ...,  1.1585,  1.1411,  1.1411],\n",
       "          [ 1.3328,  1.4897,  1.5245,  ...,  1.2457,  1.2631,  1.2631],\n",
       "          [ 1.3154,  1.5245,  1.6117,  ...,  0.8099,  0.7925,  0.7576],\n",
       "          ...,\n",
       "          [ 1.6988,  1.6988,  1.7163,  ...,  1.3154,  1.0714,  0.8971],\n",
       "          [ 1.6640,  1.6640,  1.6117,  ...,  1.5420,  0.1999, -0.5321],\n",
       "          [ 0.8448,  0.4614,  0.1302,  ...,  1.2980,  1.0191, -0.1661]]]),\n",
       " tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([[-9.2527e-01, -3.3879e-01, -3.2138e-01,  1.4676e-01,  5.2270e-01,\n",
       "          -1.5349e-01,  1.2166e+00, -2.2389e-01, -8.6560e-02,  2.8510e-01,\n",
       "           1.8474e-01,  4.2797e-01, -4.3978e+00,  2.6168e-01, -4.3044e-01,\n",
       "          -1.1920e-03,  6.6894e-01, -8.1109e-02,  5.6346e-01, -3.7477e-01,\n",
       "           1.1786e+00,  7.7134e-01, -5.2764e-01, -1.2592e+00, -4.5112e-01],\n",
       "         [-1.0690e+00,  1.4740e-01,  1.5582e-01, -4.5698e-01, -8.0204e-01,\n",
       "           4.7470e-01,  1.2425e+00, -9.0298e-01,  7.0123e-01, -6.7615e-02,\n",
       "          -1.8999e-01, -1.7142e-01, -3.1259e+00,  1.7241e-01, -3.3581e-03,\n",
       "           1.1369e+00,  4.6636e-01, -1.0841e+00,  4.3825e-01,  2.0401e-03,\n",
       "          -1.0665e+00,  7.0757e-01,  9.5706e-01, -3.5242e-01,  8.5145e-01],\n",
       "         [-6.9321e-01, -8.8635e-01, -6.4966e-01, -1.7547e-01,  1.4579e-01,\n",
       "          -3.4647e-01,  3.2935e-01, -4.9728e-01,  3.5694e-01,  5.4248e-02,\n",
       "           8.6518e-01,  3.9225e-01, -3.0987e+00, -6.7011e-01, -1.2557e-01,\n",
       "           1.0052e+00,  3.5444e-01, -1.0400e+00, -8.6182e-01,  5.8205e-01,\n",
       "          -4.5104e-01,  4.6892e-01,  6.2699e-01, -2.3306e-01, -6.2941e-01],\n",
       "         [-1.1848e+00,  2.7513e-01,  4.9909e-01,  5.8169e-01,  6.9497e-02,\n",
       "          -7.9172e-02,  6.9749e-01, -1.0830e+00, -1.6125e-01,  5.5638e-01,\n",
       "           1.9053e-01,  5.9271e-01, -3.5420e+00,  1.3479e-01, -3.6436e-01,\n",
       "           2.3156e-01, -3.5137e-01, -1.7598e-01, -2.1793e-01, -9.6499e-01,\n",
       "          -5.6965e-03,  5.3965e-01,  6.5249e-01, -6.6048e-01, -1.0342e+00],\n",
       "         [-1.4950e+00, -5.3582e-02, -4.1962e-02,  7.8463e-01, -1.6161e-03,\n",
       "          -3.8364e-01, -1.1482e+00, -1.1044e+00,  7.8190e-01, -3.8478e-01,\n",
       "           5.9273e-01,  6.0877e-02, -2.3180e-01,  2.1348e-02, -1.4821e-01,\n",
       "           4.3855e-01,  1.0387e+00,  8.1200e-01,  3.7753e-01,  1.0252e+00,\n",
       "          -4.3081e-01,  5.2436e-02,  3.1888e-01, -2.5058e-01,  1.3699e-01],\n",
       "         [-1.7714e+00, -5.9817e-01,  5.4081e-01, -4.3223e-01, -6.5592e-01,\n",
       "           8.6828e-01,  1.3332e+00,  1.7951e-01,  3.4715e-01, -7.7429e-01,\n",
       "          -5.0970e-01,  5.6861e-01, -2.5222e+00,  3.2525e-01,  2.3700e-01,\n",
       "          -8.3710e-01, -2.5715e-01, -1.3113e-01, -5.8851e-01,  2.5009e-01,\n",
       "          -8.4773e-01, -7.8738e-03, -9.2350e-02,  4.6363e-01, -4.0375e-01],\n",
       "         [-9.5101e-01,  5.0560e-01,  3.6139e-01, -4.0337e-01, -4.0042e-02,\n",
       "          -1.5647e-01,  2.2782e+00,  2.4500e-01, -4.8452e-01, -1.9628e-01,\n",
       "          -5.1955e-01,  2.1754e-01, -4.7039e+00,  9.5037e-01, -9.8481e-02,\n",
       "          -5.7552e-01,  7.2248e-02, -9.5375e-01, -6.9446e-01, -5.4825e-02,\n",
       "          -8.6766e-01,  2.5728e-02, -2.6051e-01,  7.5546e-01,  4.2688e-01],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "        dtype=torch.float64))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_loader = DataLoader(trainset, batch_size=5, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainset_loader)\n",
    "images, labels, embeddings = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 24, 25])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split train val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = random_split(trainset, (24000, 6000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 6000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set), len(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0056,  1.1187,  1.9064,  ..., -0.7137, -0.9020, -1.0733],\n",
       "          [-0.2342,  0.2282,  0.7762,  ..., -0.6623, -0.4911, -0.7993],\n",
       "          [ 0.3309,  0.3138,  0.4337,  ..., -0.8507, -1.0048, -1.0048],\n",
       "          ...,\n",
       "          [ 0.7248,  0.2282,  0.3652,  ...,  0.1254,  0.0912,  0.0912],\n",
       "          [ 0.2453,  0.1597,  0.1083,  ...,  0.1768,  0.1597,  0.1254],\n",
       "          [ 0.3138,  0.1768,  0.1254,  ...,  0.1939,  0.1426,  0.1426]],\n",
       " \n",
       "         [[ 0.3277,  1.3957,  2.1660,  ..., -0.4776, -0.6527, -0.8978],\n",
       "          [ 0.1352,  0.6429,  1.2381,  ..., -0.3375, -0.1275, -0.5476],\n",
       "          [ 0.8354,  0.8704,  1.0105,  ..., -0.6352, -0.8277, -0.7752],\n",
       "          ...,\n",
       "          [ 0.7479,  0.2927,  0.4153,  ...,  0.2227,  0.1877,  0.1702],\n",
       "          [ 0.2927,  0.2227,  0.1877,  ...,  0.2577,  0.2402,  0.1877],\n",
       "          [ 0.3803,  0.2577,  0.2052,  ...,  0.2577,  0.2052,  0.2052]],\n",
       " \n",
       "         [[ 0.5136,  1.6814,  2.3786,  ..., -0.5321, -0.7064, -0.7761],\n",
       "          [ 0.0431,  0.6008,  1.1411,  ..., -0.5321, -0.4101, -0.6193],\n",
       "          [ 0.4439,  0.3916,  0.4962,  ..., -0.6193, -0.7761, -0.7936],\n",
       "          ...,\n",
       "          [ 0.9668,  0.4962,  0.6356,  ...,  0.4788,  0.4439,  0.4439],\n",
       "          [ 0.5136,  0.4439,  0.4091,  ...,  0.4962,  0.4962,  0.4788],\n",
       "          [ 0.5834,  0.4614,  0.4091,  ...,  0.4962,  0.4614,  0.4614]]]),\n",
       " tensor([1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([[-0.0799, -0.0920, -0.4623,  0.2986,  0.7847,  0.0715,  1.5598, -0.3681,\n",
       "          -0.8971,  0.1198, -0.5836,  0.5648, -4.8192,  0.4178, -0.5212,  1.0823,\n",
       "           0.9316, -0.1566,  0.2869, -0.8490,  1.3194,  0.3609, -0.3883, -0.1788,\n",
       "          -0.8810],\n",
       "         [-1.8058,  0.6504,  0.6644, -0.2469, -0.1138,  0.9762,  0.4873, -0.7472,\n",
       "           0.2848,  0.7274, -0.1693,  0.5176, -3.3746, -0.5393,  0.1591,  1.1380,\n",
       "           0.2835, -0.7656,  0.4220,  0.1432, -0.0756,  0.4577,  1.1326, -1.5864,\n",
       "           0.1625],\n",
       "         [ 0.5805, -0.4540,  0.1898, -0.8721,  0.8744,  0.5600,  0.3076,  0.3156,\n",
       "           0.0510, -0.6413,  0.4363, -0.2356, -4.1130,  0.1109, -0.3430, -0.3463,\n",
       "          -0.1184, -0.3950, -1.1161,  0.5098, -1.0695,  0.0525, -0.1495, -0.4727,\n",
       "          -0.0078],\n",
       "         [-1.1385, -0.1675,  0.2622, -1.1043,  0.6296,  0.1068,  0.0864, -0.4830,\n",
       "           1.3433,  0.3682,  0.6140,  0.2935, -2.3589, -0.3104,  0.1585,  0.8870,\n",
       "           0.4838, -0.4135,  0.7046,  0.6462, -0.5554,  0.0619,  1.6860, -1.7912,\n",
       "          -0.3578],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000]], dtype=torch.float64))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "imshow(train_set[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available, otherwise stick with cpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Conv Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
    "        #        dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        \n",
    "        # Linear(in_features, out_features, bias=True)\n",
    "        self.fc1 = nn.Linear(500, 50)\n",
    "        self.fc2 = nn.Linear(50, 18)\n",
    "        \n",
    "        # MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "        self.max_pool = nn.MaxPool2d(2)\n",
    "        # ReLU(inplace=False)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        #print(x.shape)\n",
    "        x = self.relu(self.max_pool(self.conv2_drop(self.conv2(x))))\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1, 500)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x, torch.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LSTM net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Netrnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Netrnn, self).__init__()\n",
    "        self.lstm = nn.LSTM(25, 32, batch_first =True,bidirectional=True, dropout=0.2)\n",
    "        self.linear = nn.Linear(32*2,18)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        \n",
    "        lstm_out, (h_n,c_n) = self.lstm(sentence)\n",
    "        hidden_out =torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n",
    "        output = self.linear(hidden_out)\n",
    "        sig_output = torch.sigmoid(output)\n",
    "        \n",
    "        return output, sig_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine two nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Combine(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Combine, self).__init__()\n",
    "        self.cnn = Net()\n",
    "        self.rnn = Netrnn()\n",
    "        \n",
    "\n",
    "    def forward(self, x, embeddings):\n",
    "        output1, sig_output1 = self.cnn(x)\n",
    "        output2, sig_output2 = self.rnn(embeddings)\n",
    "        output = output1 + output2\n",
    "        sig_output = torch.sigmoid(output)\n",
    "        return output, sig_output\n",
    "    \n",
    "model = Combine().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=0)\n",
    "valset_loader = DataLoader(val_set, batch_size=1000, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valset_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "def train(epoch, log_interval=100):\n",
    "      # set training mode\n",
    "    iteration = 0\n",
    "    for ep in range(epoch):\n",
    "        model.train()\n",
    "        start = time()\n",
    "        for batch_idx, (data, target, embeddings) in enumerate(trainset_loader):\n",
    "            # bring data to the computing device, e.g. GPU\n",
    "            data, target, embeddings = data.to(device), target.to(device), embeddings.float().to(device)\n",
    "\n",
    "            # forward pass\n",
    "            output, sig_x = model(x=data, embeddings=embeddings)\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "          \n",
    "            if iteration % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    ep, batch_idx * len(data), len(trainset_loader.dataset),\n",
    "                    100. * batch_idx / len(trainset_loader), loss.item()))\n",
    "                \n",
    "            iteration += 1\n",
    "            \n",
    "        model.eval()\n",
    "        outputs, sig_output = model(x=data, embeddings=embeddings)        \n",
    "        pred_result_list = test()\n",
    "        end = time()\n",
    "        print('{:.2f}s'.format(end-start))\n",
    "    return pred_result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()  # set evaluation mode\n",
    "    f1_list = []\n",
    "    pred_result_list = []\n",
    "    with torch.no_grad():\n",
    "        for data, target, embeddings in valset_loader:\n",
    "            data = data.to(device)\n",
    "            embeddings = embeddings.float().to(device)\n",
    "            output, sig_x = model(data, embeddings)\n",
    "            f1_list.append(fbeta_score(target.numpy(), sig_x.cpu().detach().numpy()>0.5,beta=1, average='samples'))\n",
    "    f1_avg = sum(f1_list) / 6\n",
    "    print('Overall F1-Score(samples): ', f1_avg)\n",
    "    return pred_result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/24000 (0%)]\tLoss: 0.700548\n",
      "Train Epoch: 0 [6400/24000 (27%)]\tLoss: 0.203774\n",
      "Train Epoch: 0 [12800/24000 (53%)]\tLoss: 0.159026\n",
      "Train Epoch: 0 [19200/24000 (80%)]\tLoss: 0.157462\n",
      "Overall F1-Score(samples):  0.6520661375661375\n",
      "16.46s\n",
      "Train Epoch: 1 [1600/24000 (7%)]\tLoss: 0.134067\n",
      "Train Epoch: 1 [8000/24000 (33%)]\tLoss: 0.134083\n",
      "Train Epoch: 1 [14400/24000 (60%)]\tLoss: 0.127975\n",
      "Train Epoch: 1 [20800/24000 (87%)]\tLoss: 0.135514\n",
      "Overall F1-Score(samples):  0.7521550264550264\n",
      "16.16s\n",
      "Train Epoch: 2 [3200/24000 (13%)]\tLoss: 0.127032\n",
      "Train Epoch: 2 [9600/24000 (40%)]\tLoss: 0.114340\n",
      "Train Epoch: 2 [16000/24000 (67%)]\tLoss: 0.137872\n",
      "Train Epoch: 2 [22400/24000 (93%)]\tLoss: 0.117480\n",
      "Overall F1-Score(samples):  0.7813685185185185\n",
      "16.51s\n",
      "Train Epoch: 3 [4800/24000 (20%)]\tLoss: 0.110075\n",
      "Train Epoch: 3 [11200/24000 (47%)]\tLoss: 0.079895\n",
      "Train Epoch: 3 [17600/24000 (73%)]\tLoss: 0.109027\n",
      "Overall F1-Score(samples):  0.8100849206349207\n",
      "16.47s\n",
      "Train Epoch: 4 [0/24000 (0%)]\tLoss: 0.117738\n",
      "Train Epoch: 4 [6400/24000 (27%)]\tLoss: 0.097553\n",
      "Train Epoch: 4 [12800/24000 (53%)]\tLoss: 0.097730\n",
      "Train Epoch: 4 [19200/24000 (80%)]\tLoss: 0.091487\n",
      "Overall F1-Score(samples):  0.8038686507936507\n",
      "16.66s\n",
      "Train Epoch: 5 [1600/24000 (7%)]\tLoss: 0.131570\n",
      "Train Epoch: 5 [8000/24000 (33%)]\tLoss: 0.075330\n",
      "Train Epoch: 5 [14400/24000 (60%)]\tLoss: 0.100809\n",
      "Train Epoch: 5 [20800/24000 (87%)]\tLoss: 0.088784\n",
      "Overall F1-Score(samples):  0.8183041005291006\n",
      "16.61s\n",
      "Train Epoch: 6 [3200/24000 (13%)]\tLoss: 0.072888\n",
      "Train Epoch: 6 [9600/24000 (40%)]\tLoss: 0.100640\n",
      "Train Epoch: 6 [16000/24000 (67%)]\tLoss: 0.095297\n",
      "Train Epoch: 6 [22400/24000 (93%)]\tLoss: 0.113482\n",
      "Overall F1-Score(samples):  0.8167037037037037\n",
      "16.37s\n",
      "Train Epoch: 7 [4800/24000 (20%)]\tLoss: 0.076407\n",
      "Train Epoch: 7 [11200/24000 (47%)]\tLoss: 0.114159\n",
      "Train Epoch: 7 [17600/24000 (73%)]\tLoss: 0.108768\n",
      "Overall F1-Score(samples):  0.8203130952380953\n",
      "16.48s\n",
      "Train Epoch: 8 [0/24000 (0%)]\tLoss: 0.096557\n",
      "Train Epoch: 8 [6400/24000 (27%)]\tLoss: 0.089599\n",
      "Train Epoch: 8 [12800/24000 (53%)]\tLoss: 0.079721\n",
      "Train Epoch: 8 [19200/24000 (80%)]\tLoss: 0.094292\n",
      "Overall F1-Score(samples):  0.8264595238095239\n",
      "16.37s\n",
      "Train Epoch: 9 [1600/24000 (7%)]\tLoss: 0.091239\n",
      "Train Epoch: 9 [8000/24000 (33%)]\tLoss: 0.108867\n",
      "Train Epoch: 9 [14400/24000 (60%)]\tLoss: 0.088767\n",
      "Train Epoch: 9 [20800/24000 (87%)]\tLoss: 0.122888\n",
      "Overall F1-Score(samples):  0.8235398148148149\n",
      "16.48s\n",
      "Train Epoch: 10 [3200/24000 (13%)]\tLoss: 0.082317\n",
      "Train Epoch: 10 [9600/24000 (40%)]\tLoss: 0.087521\n",
      "Train Epoch: 10 [16000/24000 (67%)]\tLoss: 0.080971\n",
      "Train Epoch: 10 [22400/24000 (93%)]\tLoss: 0.108102\n",
      "Overall F1-Score(samples):  0.8219612433862434\n",
      "16.56s\n",
      "Train Epoch: 11 [4800/24000 (20%)]\tLoss: 0.098051\n",
      "Train Epoch: 11 [11200/24000 (47%)]\tLoss: 0.085533\n",
      "Train Epoch: 11 [17600/24000 (73%)]\tLoss: 0.072586\n",
      "Overall F1-Score(samples):  0.8272945767195767\n",
      "16.50s\n",
      "Train Epoch: 12 [0/24000 (0%)]\tLoss: 0.114846\n",
      "Train Epoch: 12 [6400/24000 (27%)]\tLoss: 0.112154\n",
      "Train Epoch: 12 [12800/24000 (53%)]\tLoss: 0.119896\n",
      "Train Epoch: 12 [19200/24000 (80%)]\tLoss: 0.055447\n",
      "Overall F1-Score(samples):  0.8324804232804232\n",
      "16.37s\n",
      "Train Epoch: 13 [1600/24000 (7%)]\tLoss: 0.070867\n",
      "Train Epoch: 13 [8000/24000 (33%)]\tLoss: 0.104161\n",
      "Train Epoch: 13 [14400/24000 (60%)]\tLoss: 0.084046\n",
      "Train Epoch: 13 [20800/24000 (87%)]\tLoss: 0.084185\n",
      "Overall F1-Score(samples):  0.8191075396825397\n",
      "16.41s\n",
      "Train Epoch: 14 [3200/24000 (13%)]\tLoss: 0.098612\n",
      "Train Epoch: 14 [9600/24000 (40%)]\tLoss: 0.085816\n",
      "Train Epoch: 14 [16000/24000 (67%)]\tLoss: 0.101319\n",
      "Train Epoch: 14 [22400/24000 (93%)]\tLoss: 0.126159\n",
      "Overall F1-Score(samples):  0.8281829846079846\n",
      "16.08s\n"
     ]
    }
   ],
   "source": [
    "pred_result_list = train(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
