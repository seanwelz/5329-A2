# 文件使用说明 MUST SEE !!!
colab读取本地文件困难，读取云盘文件要耗时，所以代码分成两部分解决这个问题。请先在本地运行NPY and EDA,再在colab运行CNN


# CNN.ipynb
请上传到colab上跑，开不开GPU速度差10倍，里面各部分也有小标题，自己看吧
现在train acc可以到50%了，val_acc仍然有点问题，
可能是因为我用的model是借鉴single label的，还得再改


# NPY and EDA.ipynb

请把它和train.csv,test.csv 以及文件夹data放在同一个目录下，这样就不需要改动代码中的路径了。
里面分五部分，都有小标题，如下：

1.读取csv和所有的lib

2.读取data文件夹下的几万站图片，并存储成.npy格式到你们的电脑上，生成的文件有6G，  我电脑上这个过程需要8分钟，这部分只需要运行一次即可，耐心等待。
  这个原始npy文件请保存在硬盘里，不要删。以后可以反复用。如果保存时提示内存不够，崩溃了，请分成若干个小npy分别保存，我的代码有两行是注释掉的，
  就是干这个的。不过你们的电脑高配置估计不需要。
  
3.第3部分是对上一部分生成的npy文件，检视图片高宽的范围，是EDA的一部分，可以不运行。

4.第4部分是读取原始的npy文件，resize并统一所有图片的尺寸，生成结果保存为第二个.npy文件，
  我用的是resize尺寸是32*32，所以生成的npy只有300多M，
  
  ### 你们可以随便设，以后如果对resize的尺寸不满意，在jupter上直接运行第4部分即可，不需要再运行第2部分。
  ### 请将这个npy上传到你们google drive上，路径最好是  /content/drive/My Drive 然后就可以打开 CNN.ipynb直接跑model了，别忘记开GPU。
  
5.这部分是EDA，发现标签分布是unblanced，如果需要补充的，请加在这一部分后，我原来想根据标签组合做重新做分类的，结果发现有700多种组合，
  多的几千，少的就10个。所以放弃了，还是用原始的binary coding做分类。


# 备注：
train.csv文件里的 第9086行 即9084.jpg 的caption在两个单元格里，需要合并不然读取时会报错；

如果在网上找代码，找到用tenseflow建模的，请看下代码的tenseflow版本是1还是2. 
colab上的tenseflow已经是ver2.2了，版本1的tenseflow需要改很多代码，比较麻烦，浪费时间而却不一定成功

